{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4314773-b517-4f86-bb4b-6d0771eda4f6",
   "metadata": {},
   "source": [
    "1. Write a Python script that takes a paragraph of text and performs word tokenization using NLTK. Print the list of tokens .\n",
    "\n",
    " Paragraph of text as :\n",
    " \n",
    "Mary jumps in a field and following her Sam also jumped. \n",
    "That our lives would be changed forever. The world was loud with carnage and sirens and then quiet with missing voices that would never be heard again.\n",
    "These lives remain precious to our country and infinitely precious to many of you. Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well. For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c3e3b5-2f1a-4c0d-a262-c503e73268d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'jumps', 'in', 'a', 'field', 'and', 'following', 'her', 'Sam', 'also', 'jumped', '.', 'That', 'our', 'lives', 'would', 'be', 'changed', 'forever', '.', 'The', 'world', 'was', 'loud', 'with', 'carnage', 'and', 'sirens', 'and', 'then', 'quiet', 'with', 'missing', 'voices', 'that', 'would', 'never', 'be', 'heard', 'again', '.', 'These', 'lives', 'remain', 'precious', 'to', 'our', 'country', 'and', 'infinitely', 'precious', 'to', 'many', 'of', 'you', '.', 'Today', ',', 'we', 'remember', 'your', 'loss', ',', 'we', 'share', 'your', 'sorrow', ',', 'and', 'we', 'honor', 'the', 'men', 'and', 'women', 'you', 'have', 'loved', 'so', 'long', 'and', 'so', 'well', '.', 'For', 'those', 'too', 'young', 'to', 'recall', 'that', 'clear', 'September', 'day', ',', 'it', 'is', 'hard', 'to', 'describe', 'the', 'mix', 'of', 'feelings', 'we', 'experienced', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "TEST_PARA = \"\"\"Mary jumps in a field and following her Sam also jumped. \n",
    "That our lives would be changed forever. The world was loud with carnage and sirens and then quiet with missing voices that would never be heard again.\n",
    "These lives remain precious to our country and infinitely precious to many of you. Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well. For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced. \"\"\"\n",
    "words_token = nltk.word_tokenize(TEST_PARA)\n",
    "print(words_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11f402-6ec5-4c6d-a47a-e373fc67ff52",
   "metadata": {},
   "source": [
    "2. Write a Python script that performs POS tagging on a list of tokens using NLTK. Print the list of tuples containing the word and its POS tag \n",
    "\n",
    " Paragraph of text as :\n",
    " \n",
    "Mary jumps in a field and following her Sam also jumped. \n",
    "That our lives would be changed forever. The world was loud with carnage and sirens and then quiet with missing voices that would never be heard again.\n",
    "These lives remain precious to our country and infinitely precious to many of you. Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well. For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54222bc5-7564-4041-ac23-bbb4c3c59c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mary', 'NNP'), ('jumps', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('field', 'NN'), ('and', 'CC'), ('following', 'VBG'), ('her', 'PRP$'), ('Sam', 'NNP'), ('also', 'RB'), ('jumped', 'VBD'), ('.', '.'), ('That', 'IN'), ('our', 'PRP$'), ('lives', 'NNS'), ('would', 'MD'), ('be', 'VB'), ('changed', 'VBN'), ('forever', 'RB'), ('.', '.'), ('The', 'DT'), ('world', 'NN'), ('was', 'VBD'), ('loud', 'JJ'), ('with', 'IN'), ('carnage', 'NN'), ('and', 'CC'), ('sirens', 'NNS'), ('and', 'CC'), ('then', 'RB'), ('quiet', 'JJ'), ('with', 'IN'), ('missing', 'VBG'), ('voices', 'NNS'), ('that', 'WDT'), ('would', 'MD'), ('never', 'RB'), ('be', 'VB'), ('heard', 'RB'), ('again', 'RB'), ('.', '.'), ('These', 'DT'), ('lives', 'NNS'), ('remain', 'VBP'), ('precious', 'JJ'), ('to', 'TO'), ('our', 'PRP$'), ('country', 'NN'), ('and', 'CC'), ('infinitely', 'RB'), ('precious', 'JJ'), ('to', 'TO'), ('many', 'JJ'), ('of', 'IN'), ('you', 'PRP'), ('.', '.'), ('Today', 'NN'), (',', ','), ('we', 'PRP'), ('remember', 'VBP'), ('your', 'PRP$'), ('loss', 'NN'), (',', ','), ('we', 'PRP'), ('share', 'NN'), ('your', 'PRP$'), ('sorrow', 'NN'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('honor', 'VBP'), ('the', 'DT'), ('men', 'NNS'), ('and', 'CC'), ('women', 'NNS'), ('you', 'PRP'), ('have', 'VBP'), ('loved', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('and', 'CC'), ('so', 'RB'), ('well', 'RB'), ('.', '.'), ('For', 'IN'), ('those', 'DT'), ('too', 'RB'), ('young', 'JJ'), ('to', 'TO'), ('recall', 'VB'), ('that', 'DT'), ('clear', 'JJ'), ('September', 'NNP'), ('day', 'NN'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('describe', 'VB'), ('the', 'DT'), ('mix', 'NN'), ('of', 'IN'), ('feelings', 'NNS'), ('we', 'PRP'), ('experienced', 'VBD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag\n",
    "\n",
    "TEST_PARA = \"\"\"Mary jumps in a field and following her Sam also jumped. \n",
    "That our lives would be changed forever. The world was loud with carnage and sirens and then quiet with missing voices that would never be heard again.\n",
    "These lives remain precious to our country and infinitely precious to many of you. Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well. For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced. \"\"\"\n",
    "words_token = nltk.word_tokenize(TEST_PARA)\n",
    "# Perform part-of-speech tagging\n",
    "word_pos_tag = pos_tag(words_token)\n",
    "print(word_pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8a41f-9bc5-420b-9588-db223193e77b",
   "metadata": {},
   "source": [
    "3.Write a Python script that applies stemming to a list of words using NLTK's Porter Stemmer. Print the list of stemmed words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7adbf3d-ade6-43eb-9945-ad8fbc88e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chocolates : chocol\n",
      "chocolatey : chocolatey\n",
      "choco : choco\n",
      "retrieval : retriev\n",
      "retrieved : retriev\n",
      "retrieves : retriev\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer() ## defining stemmer\n",
    "list_words = [\"chocolates\", \"chocolatey\" ,\"choco\",\"retrieval\" ,\"retrieved\" ,\"retrieves\"]\n",
    "for stem in list_words:\n",
    "    stems = ps.stem(stem)\n",
    "    print(stem, \":\",stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af18da-af00-4554-a44d-671b7acb65e7",
   "metadata": {},
   "source": [
    "4. Write a Python script that applies lemmatization to a list of words using NLTK's WordNet Lemmatize. Print the list of lemmatized words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf03a47f-4c5e-47d5-820a-868475c5c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy:\n",
      "  - Synonyms: happy, felicitous, happy, glad, happy, happy, well-chosen\n",
      "  - Antonyms: unhappy\n",
      "\n",
      "Sad:\n",
      "  - Synonyms: sad, sad, deplorable, distressing, lamentable, pitiful, sad, sorry\n",
      "  - Antonyms: glad\n",
      "\n",
      "Beautiful:\n",
      "  - Synonyms: beautiful, beautiful\n",
      "  - Antonyms: ugly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordlist = [\"happy\", \"sad\", \"beautiful\"]\n",
    "\n",
    "for word in wordlist:\n",
    "    print(f\"{word.capitalize()}:\")\n",
    "    synsets = wordnet.synsets(word)\n",
    "    synonyms = [lemma.name() for syn in synsets for lemma in syn.lemmas()]\n",
    "    #synonyms = [lemma.name() for lemma in syn.lemmas()]\n",
    "\n",
    "    antonyms = [lemma.antonyms()[0].name() for syn in synsets for lemma in syn.lemmas() if lemma.antonyms()]\n",
    "    print(f\"  - Synonyms: {', '.join(synonyms)}\")\n",
    "    print(f\"  - Antonyms: {', '.join(antonyms)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6df62-0fed-44a1-a6e0-bd9b247acb85",
   "metadata": {},
   "source": [
    "5.Write a Python script that combines word tokenization, POS tagging, stemming, and lemmatization. Print the results at each step for a given paragraph of text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f2e0d9-079d-4057-9051-6eeb50e54447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization as:\n",
      "['Durgesh', 'is', 'good', 'person.He', 'is', 'working', 'in', 'a', 'multinational', 'company', '.']\n",
      "\n",
      "Word POS TAG as:\n",
      "[('Durgesh', 'NNP'), ('is', 'VBZ'), ('good', 'JJ'), ('person.He', 'NN'), ('is', 'VBZ'), ('working', 'VBG'), ('in', 'IN'), ('a', 'DT'), ('multinational', 'JJ'), ('company', 'NN'), ('.', '.')]\n",
      "\n",
      "Word for Stemming as:\n",
      "Stemming for 'Durgesh' : durgesh\n",
      "\n",
      "Stemming for 'is' : is\n",
      "\n",
      "Stemming for 'good' : good\n",
      "\n",
      "Stemming for 'person.He' : person.h\n",
      "\n",
      "Stemming for 'is' : is\n",
      "\n",
      "Stemming for 'working' : work\n",
      "\n",
      "Stemming for 'in' : in\n",
      "\n",
      "Stemming for 'a' : a\n",
      "\n",
      "Stemming for 'multinational' : multin\n",
      "\n",
      "Stemming for 'company' : compani\n",
      "\n",
      "Stemming for '.' : .\n",
      "\n",
      "Word for Lemmatization as:\n",
      "Durgesh:\n",
      "  - Synonyms: \n",
      "  - Antonyms: \n",
      "\n",
      "Is:\n",
      "  - Synonyms: be, be, be, exist, be, be, equal, be, constitute, represent, make_up, comprise, be, be, follow, embody, be, personify, be, be, live, be, cost, be\n",
      "  - Antonyms: differ\n",
      "\n",
      "Good:\n",
      "  - Synonyms: good, good, goodness, good, goodness, commodity, trade_good, good, good, full, good, good, estimable, good, honorable, respectable, beneficial, good, good, good, just, upright, adept, expert, good, practiced, proficient, skillful, skilful, good, dear, good, near, dependable, good, safe, secure, good, right, ripe, good, well, effective, good, in_effect, in_force, good, good, serious, good, sound, good, salutary, good, honest, good, undecomposed, unspoiled, unspoilt, good, well, good, thoroughly, soundly, good\n",
      "  - Antonyms: evil, evilness, bad, badness, bad, evil, ill\n",
      "\n",
      "Person.he:\n",
      "  - Synonyms: \n",
      "  - Antonyms: \n",
      "\n",
      "Is:\n",
      "  - Synonyms: be, be, be, exist, be, be, equal, be, constitute, represent, make_up, comprise, be, be, follow, embody, be, personify, be, be, live, be, cost, be\n",
      "  - Antonyms: differ\n",
      "\n",
      "Working:\n",
      "  - Synonyms: working, workings, work, work, do_work, work, act, function, work, operate, go, run, work, work_on, process, exercise, work, work_out, make, work, work, work, work, bring, work, play, wreak, make_for, work, put_to_work, cultivate, crop, work, work, influence, act_upon, work, work, work, work, work, shape, form, work, mold, mould, forge, work, knead, work, exploit, work, solve, work_out, figure_out, puzzle_out, lick, work, ferment, work, sour, turn, ferment, work, work, working, on_the_job, working, working, running, operative, functional, working, working\n",
      "  - Antonyms: idle, malfunction\n",
      "\n",
      "In:\n",
      "  - Synonyms: inch, in, indium, In, atomic_number_49, Indiana, Hoosier_State, IN, in, in, in, in, inwards, inward\n",
      "  - Antonyms: \n",
      "\n",
      "A:\n",
      "  - Synonyms: angstrom, angstrom_unit, A, vitamin_A, antiophthalmic_factor, axerophthol, A, deoxyadenosine_monophosphate, A, adenine, A, ampere, amp, A, A, a, A, type_A, group_A\n",
      "  - Antonyms: \n",
      "\n",
      "Multinational:\n",
      "  - Synonyms: multinational, transnational\n",
      "  - Antonyms: \n",
      "\n",
      "Company:\n",
      "  - Synonyms: company, company, company, companionship, fellowship, society, company, troupe, caller, company, company, party, company, ship's_company, company, company, company, companion, accompany, keep_company\n",
      "  - Antonyms: \n",
      "\n",
      ".:\n",
      "  - Synonyms: \n",
      "  - Antonyms: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "TEST_PARA = \"Durgesh is good person.He is working in a multinational company.\"\n",
    "words_token = nltk.word_tokenize(TEST_PARA)\n",
    "print(\"Word Tokenization as:\")\n",
    "print(words_token)\n",
    "print()\n",
    "# Perform part-of-speech tagging\n",
    "print(\"Word POS TAG as:\")\n",
    "word_pos_tag = pos_tag(words_token)\n",
    "print(word_pos_tag)\n",
    "print()\n",
    "# stemming\n",
    "print(\"Word for Stemming as:\")\n",
    "ps = PorterStemmer()\n",
    "for stem in words_token:\n",
    "    stems = ps.stem(stem)\n",
    "    print(f\"Stemming for '{stem}' : {stems}\")\n",
    "    print()\n",
    "# lemmatization\n",
    "print(\"Word for Lemmatization as:\")\n",
    "\n",
    "for word in words_token:\n",
    "    print(f\"{word.capitalize()}:\")\n",
    "    synsets = wordnet.synsets(word)\n",
    "    synonyms = [lemma.name() for syn in synsets for lemma in syn.lemmas()]\n",
    "    #synonyms = [lemma.name() for lemma in syn.lemmas()]\n",
    "\n",
    "    antonyms = [lemma.antonyms()[0].name() for syn in synsets for lemma in syn.lemmas() if lemma.antonyms()]\n",
    "    print(f\"  - Synonyms: {', '.join(synonyms)}\")\n",
    "    print(f\"  - Antonyms: {', '.join(antonyms)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2831e-6624-4965-a6c0-2d6facb49f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
