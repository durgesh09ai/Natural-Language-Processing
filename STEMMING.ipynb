{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3b392a-e48b-40f8-a959-d11465625dce",
   "metadata": {},
   "source": [
    "#STEMING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c355369-2db2-4c57-9da2-e92f1abe3cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call\n",
      "caller\n",
      "call\n",
      "call\n",
      "call\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "ps = PorterStemmer() ## defining stemmer\n",
    "s_words = [\"Calls\",\"Caller\",\"Calling\",\"Call\",\"Called\"]\n",
    "for i in s_words:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12583a01-9186-44d0-9929-61ed248c1c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Life', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('journey', 'NN'), (',', ','), ('not', 'RB'), ('a', 'DT'), ('destination', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), (\"'s\", 'VBZ'), ('about', 'IN'), ('the', 'DT'), ('experiences', 'NNS'), ('we', 'PRP'), ('gather', 'VBP'), (',', ','), ('the', 'DT'), ('relationships', 'NNS'), ('we', 'PRP'), ('build', 'VBP'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('lessons', 'NNS'), ('we', 'PRP'), ('learn', 'VBP'), ('along', 'IN'), ('the', 'DT'), ('way', 'NN'), ('.', '.')]\n",
      "[('Embrace', 'NNP'), ('each', 'DT'), ('moment', 'NN'), (',', ','), ('find', 'VBP'), ('joy', 'NN'), ('in', 'IN'), ('the', 'DT'), ('simple', 'JJ'), ('things', 'NNS'), (',', ','), ('and', 'CC'), ('remember', 'VB'), ('that', 'WDT'), ('even', 'RB'), ('the', 'DT'), ('challenges', 'NNS'), ('help', 'VBP'), ('shape', 'NN'), ('who', 'WP'), ('we', 'PRP'), ('are', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    " \n",
    "document = \"Life is a journey, not a destination. It's about the experiences we gather, the relationships we build, and the lessons we learn along the way. Embrace each moment,find joy in the simple things, and remember that even the challenges help shape who we are\"\n",
    "sentences = nltk.sent_tokenize(document)   \n",
    "\n",
    "for sent in sentences: \n",
    "\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b54977c4-cc45-4bf1-9c20-5dd060efca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7c9b4f2-4bf5-47c3-95f0-7d3ebfd9ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words = {'happy', 'felicitous', 'well-chosen', 'glad'}\n",
      "{'unhappy'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    " \n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    " \n",
    "print(\"Similar words =\",set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8845696-99ce-4825-a33c-6d9023bbc7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Suyashi/NNP\n",
      "  has/VBZ\n",
      "  a/DT\n",
      "  Rabbit/NNP\n",
      "  that/WDT\n",
      "  ran/VBD\n",
      "  from/IN\n",
      "  the/DT\n",
      "  Table/NN\n",
      "  ./.\n",
      "  She/PRP\n",
      "  bought/VBD\n",
      "  it/PRP\n",
      "  from/IN\n",
      "  Isha/NNP\n",
      "  ./.\n",
      "  The/DT\n",
      "  Jumping/NNP\n",
      "  is/VBZ\n",
      "  best/JJS\n",
      "  Habit/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  Rabbit/NNP)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Sample text\n",
    "text = \"Suyashi has a Rabbit that ran from the Table. She bought it from Isha . The Jumping is best Habit of the Rabbit\"\n",
    " \n",
    "# Tokenize the text\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    " \n",
    "# Perform part-of-speech tagging\n",
    "\n",
    "word_tokens_text = pos_tag(tokens)\n",
    " \n",
    "# Define chunk grammar using regular expressions\n",
    "\n",
    "chunk_grammar = r\"\"\"\n",
    "\n",
    "    VP: {<VB.*><DT>?<JJ>*<NN>}   # Chunk sequences of verbs, determiners, adjectives, and nouns\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "# Create a chunk parser using the defined grammar\n",
    "\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    " \n",
    "# Apply chunking\n",
    "\n",
    "chunked_tokens = chunk_parser.parse(word_tokens_text)\n",
    " \n",
    "# Print the chunked tokens\n",
    "\n",
    "print(chunked_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b5cece0-e32c-44f9-acf6-4f46995c0532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Ram/NNP\n",
      "  Loves/VBZ\n",
      "  his/PRP$\n",
      "  (NP Life/NN)\n",
      "  ./.\n",
      "  He/PRP\n",
      "  have/VBP\n",
      "  (NP a/DT cat/NN)\n",
      "  named/VBN\n",
      "  RUMMY/NNP)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Sample text\n",
    "text = \"Ram Loves his Life. He have a cat named RUMMY\"\n",
    " \n",
    "# Tokenize the text\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    " \n",
    "# Perform part-of-speech tagging\n",
    "\n",
    "word_tokens_text = pos_tag(tokens)\n",
    " \n",
    "# Define chunk grammar using regular expressions\n",
    "\n",
    "chunk_grammar = r\"\"\"\n",
    "\n",
    "    NP: {<DT>?<JJ>*<NN>}    # Chunk noun phrases\n",
    "\n",
    "    PP: {<IN><NP>}           # Chunk prepositional phrases\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "# Create a chunk parser using the defined grammar\n",
    "\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    " \n",
    "# Apply chunking\n",
    "\n",
    "chunked_tokens = chunk_parser.parse(word_tokens_text)\n",
    " \n",
    "# Print the chunked tokens\n",
    "\n",
    "print(chunked_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abc22dbd-d21d-4257-8833-666375d3a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Ram/NNP\n",
      "  Loves/VBZ\n",
      "  his/PRP$\n",
      "  Life/NN\n",
      "  ./.\n",
      "  He/PRP\n",
      "  (VP have/VBP a/DT cat/NN)\n",
      "  named/VBN\n",
      "  RUMMY/NNP)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Sample text\n",
    "text = \"Ram Loves his Life. He have a cat named RUMMY\"\n",
    " \n",
    "# Tokenize the text\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    " \n",
    "# Perform part-of-speech tagging\n",
    "\n",
    "word_tokens_text = pos_tag(tokens)\n",
    " \n",
    "# Define chunk grammar using regular expressions\n",
    "\n",
    "chunk_grammar = r\"\"\"\n",
    "\n",
    "    VP: {<VB.*><DT>?<JJ>*<NN>}   # Chunk sequences of verbs, determiners, adjectives, and nouns\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "# Create a chunk parser using the defined grammar\n",
    "\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    " \n",
    "# Apply chunking\n",
    "\n",
    "chunked_tokens = chunk_parser.parse(word_tokens_text)\n",
    " \n",
    "# Print the chunked tokens\n",
    "\n",
    "print(chunked_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c79a3b-f33e-480b-87fc-30d064529c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Ram/NNP\n",
      "  Loves/VBZ\n",
      "  his/PRP$\n",
      "  (NP Life/NN)\n",
      "  ./.\n",
      "  He/PRP\n",
      "  have/VBP\n",
      "  (NP a/DT cat/NN)\n",
      "  named/VBN\n",
      "  RUMMY/NNP)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Sample text\n",
    "text = \"Ram Loves his Life. He have a cat named RUMMY\"\n",
    " \n",
    "# Tokenize the text\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    " \n",
    "# Perform part-of-speech tagging\n",
    "\n",
    "word_tokens_text = pos_tag(tokens)\n",
    " \n",
    "# Define chunk grammar using regular expressions\n",
    "\n",
    "\n",
    "\n",
    "chunk_grammar = r\"\"\"\n",
    "\n",
    "    NP: {<DT>?<JJ>*<NN>}   # Chunk sequences of DT, JJ, and NN\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "# Create a chunk parser using the defined grammar\n",
    "\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    " \n",
    "# Apply chunking\n",
    "\n",
    "chunked_tokens = chunk_parser.parse(word_tokens_text)\n",
    " \n",
    "# Print the chunked tokens\n",
    "\n",
    "print(chunked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8a4e4bd-61cc-4d8f-9fc3-5319dc2d9fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  You/PRP\n",
      "  know/VBP\n",
      "  (PERSON Ashi/NNP)\n",
      "  and/CC\n",
      "  (PERSON Raman/NNP)\n",
      "  works/VBZ\n",
      "  in/IN\n",
      "  (ORGANIZATION ABC/NNP)\n",
      "  pvt/NN\n",
      "  Lt./NNP\n",
      "  (GPE India/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  its/PRP$\n",
      "  (ORGANIZATION CEO/NNP Rommy/NNP)\n",
      "  is/VBZ\n",
      "  from/IN\n",
      "  (GPE Australia/NNP))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    " \n",
    "# Sample text\n",
    "text = \"You know Ashi and Raman works in ABC pvt Lt. India, and its CEO  Rommy  is from Australia\"\n",
    " \n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    " \n",
    "# Perform part-of-speech tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    " \n",
    "# Perform named entity recognition\n",
    "named_entities = ne_chunk(tagged_tokens)  #This function identifies named entities in the text based on the part-of-speech tags.\n",
    " \n",
    "# Print the named entities\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0c9cfc-6e7f-42fe-b9d9-641b84056d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asha - PERSON\n",
      "Bali - GPE\n",
      "Kipy - PERSON\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 33.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    " \n",
    "# Sample text\n",
    "text = \"The teacher have a kid named Asha. They stay in Bali. They have a pet named Kipy.\"\n",
    " \n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    " \n",
    "# Initialize a list to store named entities\n",
    "named_entities = []\n",
    " \n",
    "# Iterate through each sentence\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # Perform part-of-speech tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    # Perform named entity recognition\n",
    "    named_entities.extend(ne_chunk(tagged_tokens))\n",
    " \n",
    "# Print the named entities\n",
    "for entity in named_entities:\n",
    "    if hasattr(entity, 'label'):\n",
    "        print(' '.join(c[0] for c in entity.leaves()), '-', entity.label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b336ef-9fae-4dd8-8446-7409d9036b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
